{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c892250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:03:18.159475800Z",
     "start_time": "2023-11-02T09:03:18.126926800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"I love cats . cats are cute pets.\",\n",
    "    \"Dogs are loyal. Dogs are good friends.\",\n",
    "    \"Birds can sing. Birds fly in the sky.\",\n",
    "    \"Fish live underwater. Fish come in many colors.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a685610",
   "metadata": {},
   "source": [
    "In this section, we define and create unigram models for the documents. Unigrams are single words or terms, and a unigram model represents the probability distribution of individual terms in the document. The unigram_model function counts the occurrences of each term in a document, calculates the probabilities, and returns the unigram model. We create unigram models for all documents in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0e623",
   "metadata": {},
   "source": [
    "# Create Unigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d589155",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:03:20.134383700Z",
     "start_time": "2023-11-02T09:03:20.126731400Z"
    }
   },
   "outputs": [],
   "source": [
    "def unigram_model(document):\n",
    "    words = document.split()\n",
    "    total_words = len(words)\n",
    "    unigram_counts = defaultdict(int)\n",
    "    for word in words:\n",
    "        unigram_counts[word] += 1\n",
    "    unigram_model = {word: count / total_words for word, count in unigram_counts.items()}\n",
    "    return unigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c571b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:03:22.880469100Z",
     "start_time": "2023-11-02T09:03:22.872367900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create unigram models for all documents\n",
    "unigram_models = [unigram_model(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16bdec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:03:24.475898300Z",
     "start_time": "2023-11-02T09:03:24.463197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'I': 0.125,\n  'love': 0.125,\n  'cats': 0.25,\n  '.': 0.125,\n  'are': 0.125,\n  'cute': 0.125,\n  'pets.': 0.125},\n {'Dogs': 0.2857142857142857,\n  'are': 0.2857142857142857,\n  'loyal.': 0.14285714285714285,\n  'good': 0.14285714285714285,\n  'friends.': 0.14285714285714285},\n {'Birds': 0.25,\n  'can': 0.125,\n  'sing.': 0.125,\n  'fly': 0.125,\n  'in': 0.125,\n  'the': 0.125,\n  'sky.': 0.125},\n {'Fish': 0.25,\n  'live': 0.125,\n  'underwater.': 0.125,\n  'come': 0.125,\n  'in': 0.125,\n  'many': 0.125,\n  'colors.': 0.125}]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa6e564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:03:31.474208500Z",
     "start_time": "2023-11-02T09:03:31.466737600Z"
    }
   },
   "outputs": [],
   "source": [
    "#we have a query \n",
    "query = \"I like cats and dogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f84e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:03:32.990683600Z",
     "start_time": "2023-11-02T09:03:32.984362Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_query_probability(query, document_model):\n",
    "    # Tokenize the query into words\n",
    "    query_words = query.split()\n",
    "    \n",
    "    # Initialize the probability for the entire query\n",
    "    query_probability = 1.0\n",
    "    \n",
    "    # Calculate the probability for each term in the query\n",
    "    for word in query_words:\n",
    "        if word in document_model:\n",
    "            query_probability *= document_model[word]\n",
    "        else:\n",
    "            query_probability = 0.0\n",
    "            break\n",
    "    \n",
    "    return query_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25934062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:03:35.387545600Z",
     "start_time": "2023-11-02T09:03:35.381209Z"
    }
   },
   "outputs": [],
   "source": [
    "query_probability = calculate_query_probability(query, unigram_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57e1768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:03:36.902565200Z",
     "start_time": "2023-11-02T09:03:36.892666400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2641c2c",
   "metadata": {},
   "source": [
    "## Your task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "771db8aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:12:42.959828500Z",
     "start_time": "2023-11-02T09:12:42.952915900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 - Score: 0.0001 - Content: I love cats . cats are cute pets.\n",
      "Document 2 - Score: 0.0001 - Content: Dogs are loyal. Dogs are good friends.\n",
      "Document 3 - Score: 0.0001 - Content: Birds can sing. Birds fly in the sky.\n",
      "Document 4 - Score: 0.0001 - Content: Fish live underwater. Fish come in many colors.\n"
     ]
    }
   ],
   "source": [
    "# Use Laplace Smoothing for this problem\n",
    "def laplace_smoothing(documents, alpha=1):\n",
    "    # Define a small constant for smoothing\n",
    "    alpha = 1\n",
    "\n",
    "    # Create a vocabulary set to keep track of unique words\n",
    "    vocab = set()\n",
    "\n",
    "    # Count the occurrences of each word in the documents\n",
    "    word_counts = {}\n",
    "    total_word_count = 0\n",
    "\n",
    "    for document in documents:\n",
    "        for word in document.split():\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "            total_word_count += 1\n",
    "            vocab.add(word)\n",
    "\n",
    "    # Calculate probabilities with Laplace smoothing\n",
    "    word_probabilities = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        word_probabilities[word] = (word_counts.get(word, 0) + alpha) / (total_word_count + (alpha * len(vocab)))\n",
    "\n",
    "    return word_probabilities\n",
    "\n",
    "\n",
    "def search_documents(query, documents, word_probabilities):\n",
    "    query_words = query.split()\n",
    "    document_scores = {}\n",
    "\n",
    "    for i, document in enumerate(documents):\n",
    "        score = 1.0\n",
    "        for word in query_words:\n",
    "            if word in word_probabilities:\n",
    "                score *= word_probabilities[word]\n",
    "            else:\n",
    "                # Laplace smoothing for unseen words\n",
    "                score *= alpha / (total_word_count + (alpha * len(vocab)))\n",
    "\n",
    "        document_scores[i] = score\n",
    "\n",
    "    # Sort documents by score in descending order\n",
    "    sorted_documents = sorted(document_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_documents\n",
    "\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    \"I love cats . cats are cute pets.\",\n",
    "    \"Dogs are loyal. Dogs are good friends.\",\n",
    "    \"Birds can sing. Birds fly in the sky.\",\n",
    "    \"Fish live underwater. Fish come in many colors.\"\n",
    "]\n",
    "\n",
    "# Build word probabilities using Laplace smoothing\n",
    "word_probabilities = laplace_smoothing(documents)\n",
    "\n",
    "# Search for documents relevant to a query\n",
    "query = \"I love cats\"\n",
    "results = search_documents(query, documents, word_probabilities)\n",
    "\n",
    "# Print the results\n",
    "for doc_index, score in results:\n",
    "    print(f'Document {doc_index+1} - Score: {score:.4f} - Content: {documents[doc_index]}')\n",
    "\n",
    "# https://www.exploredatabase.com/2020/10/explain-add-1-laplace-smoothing-with-example.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7f2705d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T09:14:01.830213200Z",
     "start_time": "2023-11-02T09:14:01.821188100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 0.0000 - Document: Dogs are loyal. Dogs are good friends.\n",
      "Probability: 0.0000 - Document: Birds can sing. Birds fly in the sky.\n",
      "Probability: 0.0000 - Document: Fish live underwater. Fish come in many colors.\n",
      "Probability: 0.0000 - Document: I love cats . cats are cute pets.\n"
     ]
    }
   ],
   "source": [
    "# create a bigram model & apply smoothing method\n",
    "def laplace_bigram_smoothing(documents, alpha=1):\n",
    "    # Define a small constant for smoothing\n",
    "    alpha = 1\n",
    "\n",
    "    # Create a vocabulary set to keep track of unique words\n",
    "    vocab = set()\n",
    "\n",
    "    # Count the occurrences of each bigram in the documents\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "\n",
    "    for document in documents:\n",
    "        words = document.split()\n",
    "        for i in range(len(words)-1):\n",
    "            bigram = (words[i], words[i+1])\n",
    "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
    "            unigram_counts[words[i]] = unigram_counts.get(words[i], 0) + 1\n",
    "            vocab.add(words[i])\n",
    "\n",
    "    # Calculate probabilities with Laplace smoothing\n",
    "    bigram_probabilities = {}\n",
    "\n",
    "    for bigram in bigram_counts:\n",
    "        prev_word = bigram[0]\n",
    "        bigram_probabilities[bigram] = (bigram_counts.get(bigram, 0) + alpha) / (unigram_counts.get(prev_word, 0) + (alpha * len(vocab)))\n",
    "\n",
    "    return bigram_probabilities, vocab\n",
    "\n",
    "\n",
    "def search_documents_bigram(query, documents, bigram_model):\n",
    "    bigram_probabilities, vocab = bigram_model\n",
    "    query_words = query.split()\n",
    "    relevant_documents = []\n",
    "\n",
    "    for i, document in enumerate(documents):\n",
    "        document_probability = 1.0\n",
    "        words = document.split()\n",
    "        for j in range(len(words)-1):\n",
    "            bigram = (words[j], words[j+1])\n",
    "            if bigram in bigram_probabilities:\n",
    "                document_probability *= bigram_probabilities[bigram]\n",
    "            else:\n",
    "                # Laplace smoothing for unseen bigrams\n",
    "                document_probability *= alpha / (unigram_counts.get(prev_word, 0) + (alpha * len(vocab)))\n",
    "\n",
    "        relevant_documents.append((document, document_probability))\n",
    "\n",
    "    # Sort documents by probability in descending order\n",
    "    relevant_documents.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return relevant_documents\n",
    "\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    \"I love cats . cats are cute pets.\",\n",
    "    \"Dogs are loyal. Dogs are good friends.\",\n",
    "    \"Birds can sing. Birds fly in the sky.\",\n",
    "    \"Fish live underwater. Fish come in many colors.\"\n",
    "]\n",
    "\n",
    "# Build the bigram model with Laplace smoothing\n",
    "bigram_model = laplace_bigram_smoothing(documents)\n",
    "\n",
    "# Search for documents relevant to a query using bigram model\n",
    "query = \"I love cats\"\n",
    "results = search_documents_bigram(query, documents, bigram_model)\n",
    "\n",
    "# Print the results\n",
    "for document, probability in results:\n",
    "    print(f'Probability: {probability:.4f} - Document: {document}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T09:09:27.040452Z",
     "start_time": "2023-11-02T09:09:27.032717600Z"
    }
   },
   "id": "89a631db02e20fbe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
